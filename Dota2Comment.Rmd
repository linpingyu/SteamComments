---
title: "TextAnalysisOnDota2Comments"
author: "Linping"
date: "4/17/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

```{r}
library(tidyverse)
library(lubridate)
```

```{r}
library(tidytext)
```

**tidy text data**
```{r}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")
text_df <- data_frame(line = 1:4, text = text)
text_df %>% 
  unnest_tokens(word, text) # first arg is new column name

library(janeaustenr)
library(stringr)
austen_books()
original_books <- austen_books() %>% 
  group_by(book) %>% 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex('^chapter [\\divxlc]', 
                                                 ignore_case = TRUE)))) %>% 
  ungroup()
original_books

tidy_books <- original_books %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
tidy_books

tidy_books %>% 
  count(word, sort = TRUE)

library(gutenbergr)

hgwells <- gutenberg_download(c(35, 36, 5230, 159))
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)

library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)

cor.test(data=frequency[frequency$author == 'H.G. Wells', ], ~proportion + `Jane Austen`)

cor.test(data=frequency[frequency$author == "Brontë Sisters",], ~proportion + `Jane Austen`)
```

**sentiment analysis**
```{r}
tidy_books <- austen_books() %>% 
  group_by(book) %>% 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex('^chapter [\\divxlc]', ignore_case = T)))) %>% 
  ungroup() %>% 
  unnest_tokens(word, text)

nrc_joy <- get_sentiments('nrc') %>% 
  filter(sentiment == 'joy')
tidy_books %>% 
  filter(book == 'Emma') %>% 
  inner_join(nrc_joy) %>% 
  count(word, sort = T)

jane_austen_sentiment <- tidy_books %>% 
  inner_join(get_sentiments('bing')) %>% 
  count(book, index = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = F) +
  facet_wrap(~book, ncol = 2, scales = 'free_x')


pride_prejudice <- tidy_books %>% 
  filter(book == 'Pride & Prejudice')
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments('afinn')) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = 'AFINN')
bing_nrc_loughran <- bind_rows(pride_prejudice %>%
                                 inner_join(get_sentiments('bing')) %>% 
                                 mutate(method = 'BING'),
                               pride_prejudice %>% 
                                 inner_join(get_sentiments('nrc')) %>% 
                                 filter(sentiment %in% c("positive", "negative")) %>%
                                 mutate(method = "NRC"), 
                               pride_prejudice %>% 
                                 inner_join(get_sentiments('loughran')) %>% 
                                 filter(sentiment %in% c('positive', 'negative')) %>% 
                                 mutate(method = 'LOUGHRAN')) %>% 
  count(method, index = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% mutate(sentiment = positive - negative) %>% select(-positive,-negative)
bind_rows(afinn, bing_nrc_loughran) %>% 
  ggplot(aes(index, sentiment, fill = method))+ 
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")


library(wordcloud)
tidy_books %>% 
  anti_join(stop_words) %>% 
  count(word) %>% with(wordcloud(word, n, random.color = T, max.words = 100))


library(reshape2)
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c('gray20','gray80'), max.words = 100)

PandP_sentences <- data_frame(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = 'sentences')

bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == 'negative')
wordcounts <- tidy_books %>% 
  group_by(book, chapter) %>% 
  summarise(words = n())
tidy_books %>% 
  semi_join(bingnegative) %>% 
  group_by(book, chapter) %>% 
  summarise(negativewords = n()) %>% 
  left_join(wordcounts, by = c('book','chapter')) %>% 
  mutate(ratio = negativewords / words) %>% 
  filter(chapter != 0) %>% 
  top_n(1) %>% 
  ungroup()
```

**tf-idf (term-frequency-inverse document frequency)**
```{r}
book_words <- austen_books() %>% 
  unnest_tokens(word, text) %>% 
  count(book, word, sort = TRUE) %>% 
  ungroup() 

total_words <- book_words %>% 
  group_by(book) %>% 
  summarise(total = sum(n))

book_words <- left_join(book_words, total_words)
head(book_words,10)
ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend=FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol=2, scales='free_y')

freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(),
         `term frequency` = n/total)
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) +
  geom_line(size = 1.1, alpha = 0.6, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

rank_subset <- freq_by_rank %>% 
  filter(rank < 500, rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

book_words <- book_words %>% 
  bind_tf_idf(word, book, n)
book_words %>% 
  select(-total) %>% 
  arrange(desc(tf_idf))

book_words %>% 
  arrange(desc(tf_idf)) %>% 
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup() %>% 
  ggplot(aes(word, tf_idf, fill = book)) + 
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = 'tf_idf') +
  facet_wrap(~book, ncol=2, scales='free') +
  coord_flip()

physics <- gutenberg_download(c(37729, 14725, 13476, 5001), 
                              meta_fields = "author")

physics_words <- physics %>% 
  unnest_tokens(word, text) %>% 
  count(author, word, sort = TRUE) %>% 
  ungroup()
head(physics_words)

plot_physics <- physics_words %>% 
  bind_tf_idf(word, author, n) %>% 
  arrange(desc(tf_idf)) %>% 
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  mutate(author = factor(author, levels = c('Galilei, Galileo',
                                            "Huygens, Christiaan", 
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))
plot_physics %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf, fill=author)) + 
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = 'tf-idf') +
  facet_wrap(~author, ncol=2, scales='free') +
  coord_flip()

physics_stopwords <- data_frame(word = c('eq','co','rc','ac','ak','bn','fig',
                                         'file','cg','cb','cm'))
physics_words <- anti_join(physics_words, physics_stopwords, by = 'word')
plot_physics <- physics_words %>% 
  bind_tf_idf(word, author, n) %>% 
  arrange(desc(tf_idf)) %>% 
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>% 
  mutate(author = factor(author, levels = c("Galilei, Galileo", "Huygens, Christiaan", "Tesla, Nikola", "Einstein, Albert")))

ggplot(plot_physics, aes(word, tf_idf, fill=author)) + 
  geom_col(show.legend = FALSE) + 
  labs(x = NULL, y='tf-idf') + 
  facet_wrap(~author, ncol=2,scales='free') +
  coord_flip()
```

**n-grams tokenization**
```{r}
austen_bigrams <- austen_books() %>% 
  unnest_tokens(bigram, text, token = 'ngrams', n= 2)
austen_bigrams %>% count(bigram, sort=TRUE)
bigrams_separated <- austen_bigrams %>% 
  separate(bigram, c('word1', 'word2'), sep=' ')
bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)
length(stop_words$word)
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort=TRUE)
bigram_counts
bigrams_united <- bigrams_filtered %>% 
  unite(bigram, word1, word2, sep=' ')
bigrams_united %>% 
  count(bigram, sort=TRUE)

austen_books() %>% 
  unnest_tokens(trigram, text, token='ngrams',n=3) %>% 
  separate(trigram, c('word1','word2','word3'), sep=' ') %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>% 
  count(word1, word2, word3, sort = TRUE)

bigrams_filtered %>% 
  filter(word2 == 'street') %>% 
  count(book, word1, sort = TRUE)

bigram_tf_idf <- bigrams_united %>% 
  count(book, bigram) %>% 
  bind_tf_idf(bigram, book, n) %>% 
  arrange(desc(tf_idf))
bigram_tf_idf

bigram_tf_idf %>% 
  group_by(book) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>% 
  mutate(bigram = reorder(bigram, tf_idf)) %>% 
  ggplot(aes(bigram, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = 'tf-idf') +
  facet_wrap(~book, ncol=2, scales = 'free') +
  coord_flip()

bigrams_separated %>% 
  filter(word1 == 'not') %>% 
  count(word1, word2, sort = TRUE)

not_words <- bigrams_separated %>% 
  filter(word1 == 'not') %>% 
  inner_join(get_sentiments('afinn'), by = c('word2' = 'word')) %>% 
  count(word2, score, sort = TRUE) %>% 
  ungroup()

not_words %>% 
  mutate(contribution = n * score) %>% 
  arrange(desc(abs(contribution))) %>% 
  head(20) %>% 
  mutate(word2 = reorder(word2, contribution)) %>% 
  ggplot(aes(word2, n*score,fill=n*score > 0))+
  geom_col(show.legend = FALSE) +
  labs(x='Words preceded by \"not\"', y = 'Sentiment score * number of occurrences') +
  coord_flip()

library(igraph)
bigram_graph <- bigram_counts %>% 
  filter(n > 20) %>% 
  graph_from_data_frame()
bigram_graph

library(ggraph)
set.seed(2017)
a <- grid::arrow(type = 'closed', length=unit(.15, 'inches'))

ggraph(bigram_graph, layout = 'fr') +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches'))+
  geom_node_point(color = 'lightblue', size=5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)+
  theme_void()

count_bigrams <- function(dataset) {
  dataset %>% 
    unnest_tokens(bigram, text, token = 'ngrams', n=2) %>% 
    separate(bigram, c('word1','word2'), sep=' ') %>% 
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>% 
    count(word1, word2, sort = TRUE)
}
visualize_bigrams <- function(bigrams){
  set.seed(2018)
  a <- grid::arrow(type = 'closed', length = unit(.15, 'inches'))
  bigrams %>% 
    graph_from_data_frame() %>% 
    ggraph(layout = 'fr') +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = 'lightblue',size=5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

kjv <- gutenberg_download(10)
kjv_bigrams <- kjv %>% 
  count_bigrams()
kjv_bigrams %>% 
  filter(n > 40, 
         !str_detect(word1, '\\d'),
         !str_detect(word2, '\\d')) %>% 
  visualize_bigrams()

austen_section_words <- austen_books() %>% 
  filter(book == 'Pride & Prejudice') %>% 
  mutate(section = row_number() %/% 10) %>% 
  filter(section > 0) %>% 
  unnest_tokens(word, text) %>% 
  filter(!word %in% stop_words$word)

austen_section_words

library(widyr)
word_pairs <- austen_section_words %>% 
  pairwise_count(word, section, sort = TRUE)
word_pairs

word_pairs %>% filter(item1 == 'darcy')
word_cors <- austen_section_words %>% 
  group_by(word) %>% 
  filter(n() >= 20) %>% 
  pairwise_cor(word, section, sort = TRUE)
word_cors %>% 
  filter(item1 == 'pounds')
```

**converting to and from non-tidy formats**
```{r}
library(tm)
data("AssociatedPress", package = "topicmodels")
AssociatedPress
terms <- Terms(AssociatedPress)
head(terms)

ap_td <- tidy(AssociatedPress)

ap_sentiments <- ap_td %>% 
  inner_join(get_sentiments('bing'), by=c('term'='word'))

ap_sentiments %>% 
  count(sentiment, term, wt=count) %>% 
  ungroup() %>% 
  filter(n >= 200) %>% 
  mutate(n = ifelse(sentiment == 'negative', -n, n)) %>% 
  mutate(term = reorder(term, n)) %>% 
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = 'identity') +
  ylab('Contribution to sentiment') +
  coord_flip()

data('data_corpus_inaugural', package = 'quanteda')
inaug_dfm <- quanteda::dfm(data_corpus_inaugural, verbose = FALSE)
inaug_dfm

inaug_td <- tidy(inaug_dfm)
inaug_td

inaug_tf_idf <- inaug_td %>% 
  bind_tf_idf(term, document, count) %>% 
  arrange(desc(tf_idf))
inaug_tf_idf %>% 
  filter(document %in% c('1861-Lincoln', '1933-Roosevelt', '1961-Kennedy', '2009-Obama')) %>% 
  group_by(document) %>% 
  top_n(10, tf_idf) %>% 
  ungroup() %>% 
  mutate(term = reorder(term, tf_idf)) %>% 
  ggplot(aes(x=term, y=tf_idf, fill=document))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~document, ncol=2, scales='free')+
  coord_flip()

year_term_counts <- inaug_td %>% 
  extract(document, 'year', '(\\d+)', convert = TRUE) %>% 
  complete(year, term, fill = list(count = 0)) %>% 
  group_by(year) %>% 
  mutate(year_total = sum(count))

year_term_counts %>% 
  filter(term %in% c('god','america','foreign','union','constitution','freedom')) %>% 
  ggplot(aes(year, count / year_total))+
  geom_point() +
  geom_smooth()+
  facet_wrap(~term, scales = 'free_y')+
  scale_y_continuous(labels = scales::percent_format())+
  ylab('% frequency of word in inaugural address')

ap_td %>% 
  cast_dtm(document, term, count)

ap_td %>% 
  cast_dfm(document, term, count)

library(Matrix)
m <- ap_td %>% 
  cast_sparse(document, term, count)
class(m)
dim(m)

austen_dtm <- austen_books() %>% 
  unnest_tokens(word, text) %>% 
  count(book, word) %>% 
  cast_dtm(book, word, n)
austen_dtm

data('acq')
acq_td <- tidy(acq)
acq_tokens <- acq_td %>% 
  select(-places) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = 'word')

acq_tokens %>% 
  count(word, sort=TRUE)
acq_tokens %>% 
  count(id, word) %>% 
  bind_tf_idf(word, id, n) %>% 
  arrange(desc(tf_idf))
```

**topic modeling**
```{r}
library(topicmodels)

# per-topic-per-word
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_topics <- tidy(ap_lda, matrix = 'beta')
ap_top_terms <- ap_topics %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)
ap_top_terms %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill=factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales='free')+
  coord_flip()
beta_spread <- ap_topics %>% 
  mutate(topic = paste0('topic',topic)) %>% 
  spread(topic, beta) %>% 
  filter(topic1 > .001|topic2>.001) %>% 
  mutate(log_ratio = log2(topic2 / topic1))
beta_spread %>% 
  top_n(20, abs(log_ratio)) %>% 
  mutate(term = reorder(term, log_ratio)) %>% 
  ggplot(aes(term, log_ratio))+
  geom_col(show.legend = FALSE)+
  coord_flip()

# per-document-per-topic
ap_documents <- tidy(ap_lda, matrix='gamma')
tidy(AssociatedPress) %>% 
  filter(document == 6) %>% 
  arrange(desc(count))

titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
            "Pride and Prejudice", "Great Expectations")
books <- gutenberg_works(title %in% titles) %>% 
  gutenberg_download(meta_fields = 'title')

by_chapter <- books %>% 
  group_by(title) %>% 
  mutate(chapter = cumsum(str_detect(text, regex('^chapter ', ignore_case = TRUE)))) %>% 
  ungroup() %>% 
  filter(chapter > 0) %>% 
  unite(document, title, chapter)

by_chapter_word <- by_chapter %>% 
  unnest_tokens(word, text)

word_counts <- by_chapter_word %>% 
  anti_join(stop_words) %>% 
  count(document, word, sort=TRUE) %>% 
  ungroup()

chapters_dtm <- word_counts %>% 
  cast_dtm(document, word, n)
inspect(chapters_dtm)

chapters_lda <- LDA(chapters_dtm, k=4, control=list(seed=1234))
chapter_topics <- tidy(chapters_lda, matrix='beta')

top_terms <- chapter_topics %>% 
  group_by(topic) %>% 
  top_n(5, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

top_terms %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = 'free') +
  coord_flip()

chapters_gamma <- tidy(chapters_lda, matrix = 'gamma')
chapters_gamma <- chapters_gamma %>% 
  separate(document, c('title','chapter'), sep = '_', convert = TRUE)

chapters_gamma %>% 
  mutate(title = reorder(title, gamma * topic)) %>% 
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~title)

chapter_classifications <- chapters_gamma %>% 
  group_by(title, chapter) %>% 
  top_n(1, gamma) %>% 
  ungroup()

book_topics <- chapter_classifications %>% 
  count(title, topic) %>% 
  group_by(title) %>% 
  top_n(1, n) %>% 
  ungroup() %>% 
  transmute(consensus = title, topic)

chapter_classifications %>% 
  inner_join(book_topics, by = 'topic') %>% 
  filter(title != consensus)

assignments <- augment(chapters_lda, data = chapters_dtm)

assignments <- assignments %>% 
  separate(document, c('title','chapter'),sep='_',convert = T) %>% 
  inner_join(book_topics, by = c('.topic'='topic'))

assignments %>% 
  count(title, consensus, wt = count) %>% 
  group_by(title) %>% 
  mutate(percent = n / sum(n)) %>% 
  ggplot(aes(consensus, title, fill=percent)) +
  geom_tile() +
  scale_fill_gradient2(high = 'red', label=percent_format()) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = 'Book words were assigned to', 
       y = 'Book words came from',
       fill = '% of assignments')

wrong_words <- assignments %>% 
  filter(title != consensus)

wrong_words %>% 
  count(title, consensus, term, wt=count) %>% 
  ungroup() %>% 
  arrange(desc(n))
```





